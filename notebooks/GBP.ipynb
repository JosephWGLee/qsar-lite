{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4a56b1-b8f4-4aff-8356-9a08f1c9309e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading some Pytorch utilities, missing a dependency. No module named 'torch'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This module requires PyTorch to be installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jofu2\\anaconda3\\envs\\deepchem-env\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading some PyTorch models, missing a dependency. No module named 'torch'\n",
      "No module named 'torch'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch'\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'torch'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import deepchem as dc\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from rdkit import Chem\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "\n",
    "import warnings\n",
    "from rdkit import RDLogger\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a74de13-3b24-45b0-aab4-b3daf04e49c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing ESOL Delaney solubility dataset \n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    tasks, datasets, transformers = dc.molnet.load_delaney(featurizer=\"GraphConv\", splitter=\"random\", reload=False)\n",
    "    \n",
    "    train_dataset, valid_dataset, test_dataset = datasets\n",
    "    \n",
    "    train_df = train_dataset.to_dataframe()\n",
    "    valid_df = valid_dataset.to_dataframe()\n",
    "    test_df = test_dataset.to_dataframe()\n",
    "    \n",
    "    smiles_train, sol_train = train_df[\"ids\"], train_df[\"y\"]\n",
    "    #smiles_test, sol_test = test_df[\"ids\"], test_df[\"y\"]\n",
    "    #smiles_valid, sol_valid = valid_df[\"ids\"], valid_df[\"y\"]\n",
    "\n",
    "    return smiles_train, sol_train #(smiles_valid, sol_valid), (smiles_test, sol_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1da648f-ea0a-444c-bcdd-bdc931d034b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Featurise data\n",
    "\n",
    "def featurise_data(smiles, solubility):\n",
    "    \n",
    "    featurizer = dc.feat.CircularFingerprint(size=2048, radius=4)\n",
    "    \n",
    "    X = featurizer.featurize(smiles)\n",
    "    y = solubility\n",
    "\n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8691a157-b298-4605-8d87-f33e07162dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split \n",
    "\n",
    "def splitting(X, y, smiles):\n",
    "\n",
    "    seed = 400\n",
    "    random.seed(seed)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, smiles_train, smiles_test = train_test_split(X, y, smiles, test_size=0.2, random_state=seed)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, smiles_train, smiles_test, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f275cfaf-d374-4414-9b41-ddf98a0c4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "\n",
    "def first_training(X_train, y_train, seed):\n",
    "\n",
    "    reg = GradientBoostingRegressor(random_state=seed)\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    score=(cross_val_score(reg, X_train, y_train, cv=3, n_jobs=-1).mean())\n",
    "    print(f\"Initial Cross-validation score is: {score}\")\n",
    "\n",
    "    return score, reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ccb40a-868a-4fe9-87bf-98e0d34a3a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional Optimisation\n",
    "\n",
    "def optional_optimisation(reg, X_train, y_train):\n",
    "\n",
    "    #Example optimisation parameters. For exhaustive searches use a range of values. \n",
    "    \n",
    "    optimisation_param_grid = {\n",
    "        \"n_estimators\": [500],\n",
    "        \"learning_rate\": [0.1],\n",
    "        \"max_depth\":[7],\n",
    "        \"min_samples_leaf\": [4],\n",
    "        \"min_samples_split\": [6], \n",
    "        \"subsample\": [0.7]\n",
    "    }\n",
    "    \n",
    "    reg2 = GridSearchCV(reg, optimisation_param_grid, cv=3, n_jobs=-1)\n",
    "    \n",
    "    search = RandomizedSearchCV(reg, param_distributions= optimisation_param_grid, n_iter=100, cv=3, n_jobs=-1, verbose=2)\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = search.best_params_\n",
    "    best_optimisation_score = search.best_score_\n",
    "    \n",
    "    print(f\"Best Training Parametres: {best_params}\")\n",
    "    print(f\"Best Training Score: {best_optimisation_score}\")\n",
    "\n",
    "    return search, reg2, best_params, best_optimisation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e672ee49-0142-4e79-97fd-5084a3692200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final training  \n",
    "\n",
    "def final_training(X_train, y_train, best_params, seed=400):\n",
    "\n",
    "    reg = GradientBoostingRegressor(\n",
    "        n_estimators=best_params[\"n_estimators\"],\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        max_depth=best_params[\"max_depth\"],\n",
    "        min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "        min_samples_split=best_params[\"min_samples_split\"],\n",
    "        subsample=best_params[\"subsample\"],\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    reg.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Final model trained on full training data\")\n",
    "\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b90bfa51-14dd-41d3-81d8-6d05e1961a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframes\n",
    "\n",
    "def dataframe_creation(reg, y_test, smiles_test, X_train, y_train, smiles_train):\n",
    "\n",
    "    def build_df(X, y, smiles):\n",
    "        preds = reg.predict(X)\n",
    "        residuals = preds - y \n",
    "        return pd.DataFrame({\"SMILES\":smiles, \n",
    "                             \"Actual Solubility logS [mol/L]\": y,\n",
    "                             \"Predicted Solubility logS [mol/L]\": preds,\n",
    "                             \"Residuals\": residuals\n",
    "                            })\n",
    "\n",
    "    train_dataframe = build_df(X_train, y_train, smiles_train)\n",
    "    test_dataframe = build_df(X_test, y_test, smiles_test)\n",
    "                             \n",
    "    return test_dataframe, train_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99cdb626-a78b-442d-82f7-167e0d99fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Scores\n",
    "\n",
    "def score_calculation(test_dataframe, train_dataframe):\n",
    "\n",
    "    predicted_RMSE = root_mean_squared_error(test_dataframe[\"Actual Solubility logS [mol/L]\"], test_dataframe[\"Predicted Solubility logS [mol/L]\"])\n",
    "    predicted_r2_score = r2_score(test_dataframe[\"Actual Solubility logS [mol/L]\"], test_dataframe[\"Predicted Solubility logS [mol/L]\"])\n",
    "    \n",
    "    print(f\"The predicted RMSE is: {predicted_RMSE}\")\n",
    "    print(f\"The predicted R2 score is: {predicted_r2_score}\")\n",
    "\n",
    "    return predicted_RMSE, predicted_r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec153507-a62b-440b-825d-2203a18545a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter graph \n",
    "\n",
    "def scatter_plot(test_dataframe, predicted_RMSE, predicted_r2_score):\n",
    "\n",
    "    min_val = test_dataframe[\"Actual Solubility logS [mol/L]\"].min()\n",
    "    max_val = test_dataframe[\"Actual Solubility logS [mol/L]\"].max()\n",
    "    \n",
    "    plt.scatter(test_dataframe[\"Actual Solubility logS [mol/L]\"], test_dataframe[\"Predicted Solubility logS [mol/L]\"], s=3) #x, y\n",
    "    plt.title(\"Predicted molecular solubility vs measured solubility using gradient boosted trees\")\n",
    "    plt.xlabel(\"Measured Solubility logS [mol/L]\")\n",
    "    plt.ylabel(\"Predicted Solubility logS [mol/L]\")\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--')\n",
    "    plt.text(0.9, 0.2, 'R-squared = %.3f\\nRMSE = %.3f' % (predicted_r2_score, predicted_RMSE))\n",
    "    plt.savefig(\"Scatter Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d06667b4-6ce9-4925-8fd2-3bd21932fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual Plot \n",
    "\n",
    "def residual_plot(test_dataframe):\n",
    "\n",
    "    sns.residplot(x=test_dataframe[\"Predicted Solubility logS [mol/L]\"], y=test_dataframe[\"Residuals\"])\n",
    "    plt.title(\"Residual Plot\")\n",
    "    plt.xlabel(\"Predicted Solubility logS [mol/L]\")\n",
    "    plt.ylabel(\"Residuals (Predicted - Actual)\")\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.savefig(\"Residual Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b90079f-9d58-45a1-b71f-2caf3c99cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance\n",
    "\n",
    "def feature_importance(reg, X_train):\n",
    "\n",
    "    importances = reg.feature_importances_ #GB assigns numerical importance to each feature\n",
    "    X_train_df = pd.DataFrame(X_train, columns=[f\"Feature {i}\" for i in range(X_train.shape[1])])\n",
    "    feature_names = X_train_df.columns\n",
    "    \n",
    "    feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.barh(feat_imp_df['Feature'][:20], feat_imp_df['Importance'][:20])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 20 Feature Importances')\n",
    "    plt.savefig(\"Feature Importance Plot\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b05364-ff4e-4390-8ebb-f36abbb1f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the code \n",
    "\n",
    "smiles_train, sol_train = load_data()\n",
    "X, y = featurise_data(smiles_train, sol_train)\n",
    "X_train, X_test, y_train, y_test, smiles_train, smiles_test, seed = splitting(X, y, smiles_train)\n",
    "score, reg = first_training(X_train, y_train, seed)\n",
    "search, reg2, best_params, best_optimisation_score = optional_optimisation(reg, X_train, y_train)\n",
    "reg = final_training(X_train, y_train, best_params, seed=400)\n",
    "test_dataframe, train_dataframe = dataframe_creation(reg, y_test, smiles_test, X_train, y_train, smiles_train)\n",
    "predicted_RMSE, predicted_r2_score = score_calculation(test_dataframe, train_dataframe)\n",
    "scatter_plot(test_dataframe, predicted_RMSE, predicted_r2_score)\n",
    "residual_plot(test_dataframe)\n",
    "feature_importance(reg, X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepchem-env)",
   "language": "python",
   "name": "deepchem-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
